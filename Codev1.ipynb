{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jyoti.prakash\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "#Importing packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm_notebook\n",
    "from nltk import word_tokenize\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from nltk import word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the files\n",
    "\n",
    "train= pd.read_csv(\"train.csv\")\n",
    "test= pd.read_csv(\"test.csv\")\n",
    "sample_sub=pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 6)\n",
      "(2345796, 3)\n"
     ]
    }
   ],
   "source": [
    "#Size of the dataframes\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              0\n",
       "qid1            0\n",
       "qid2            0\n",
       "question1       1\n",
       "question2       2\n",
       "is_duplicate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Missing Values count\n",
    "\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "5   5    11    12  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6   6    13    14                                Should I buy tiago?   \n",
       "7   7    15    16                     How can I be a good geologist?   \n",
       "8   8    17    18                    When do you use シ instead of し?   \n",
       "9   9    19    20  Motorola (company): Can I hack my Charter Moto...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...             1  \n",
       "6  What keeps childern active and far from phone ...             0  \n",
       "7          What should I do to be a great geologist?             1  \n",
       "8              When do you use \"&\" instead of \"and\"?             0  \n",
       "9  How do I hack Motorola DCX3400 for free internet?             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove the entire row if any column has missing values\n",
    "\n",
    "train=train.dropna(how='any').reset_index(drop=True)\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    255024\n",
       "1    149263\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Target Label Counts\n",
    "\n",
    "train['is_duplicate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question1', 'question2', 'is_duplicate'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop ID, qid1,qid2\n",
    "\n",
    "train=train.drop(['id','qid1','qid2'],axis=1)\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the step by step guide to invest in share market in india?\n",
      "What is the step by step guide to invest in share market?\n",
      "How can I be a good geologist?\n",
      "What should I do to be a great geologist?\n"
     ]
    }
   ],
   "source": [
    "#Sample examples from each class\n",
    "\n",
    "ques1= train.iloc[0,0]\n",
    "ques2= train.iloc[0,1]\n",
    "\n",
    "ques3= train.iloc[7,0]\n",
    "ques4= train.iloc[7,1]\n",
    "\n",
    "print(ques1)\n",
    "print(ques2)\n",
    "\n",
    "print(ques3)\n",
    "print(ques4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques1=ques1.lower().split()\n",
    "ques2=ques2.lower().split()\n",
    "ques3=ques3.lower().split()\n",
    "ques4=ques4.lower().split()\n",
    "\n",
    "\n",
    "ques1= [i for i in ques1 if i not in stop_words]\n",
    "ques2= [i for i in ques2 if i not in stop_words]\n",
    "ques3= [i for i in ques3 if i not in stop_words]\n",
    "ques4= [i for i in ques4 if i not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence length based features \n",
    "train['len_q1']= train['question1'].apply(lambda x: len(str(x)))\n",
    "train['len_q2']= train['question2'].apply(lambda x: len(str(x)))\n",
    "train['len_diff'] = train['len_q1'] - train['len_q2']\n",
    "\n",
    "# character length based features\n",
    "train['len_char_q1'] = train.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "train['len_char_q2'] = train.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "\n",
    "# word length based features\n",
    "train['len_word_q1'] = train.question1.apply(lambda x: len(str(x).split()))\n",
    "train['len_word_q2'] = train.question2.apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# common words in the two questions\n",
    "train['common_words'] = train.apply(lambda x: \n",
    "len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features set 1\n",
    "\n",
    "fs_1= ['len_q1','len_q2', 'len_char_q1','len_char_q2','len_word_q2','len_word_q2','common_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FuzzyWuzzy is a library of Python which is used for string matching. Fuzzy string matching is the process of finding strings that match a given pattern. Basically it uses Levenshtein Distance to calculate the differences between sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features from FuzzyWuzzy\n",
    "\n",
    "train['fuzz_qratio'] = train.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "train['fuzz_WRatio'] = train.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "train['fuzz_partial_ratio'] = train.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "train['fuzz_partial_token_set_ratio'] = train.apply(lambda x:fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "train['fuzz_partial_token_sort_ratio'] = train.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "train['fuzz_token_set_ratio'] = train.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "train['fuzz_token_sort_ratio'] = train.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "train['fuzz_token_sort_ratio'] = train.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickle dumped\n",
    "\n",
    "infile = open(filename,'rb')\n",
    "train = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question1', 'question2', 'is_duplicate', 'len_q1', 'len_q2',\n",
       "       'len_diff', 'len_char_q1', 'len_char_q2', 'len_word_q1', 'len_word_q2',\n",
       "       'common_words', 'fuzz_qratio', 'fuzz_WRatio', 'fuzz_partial_ratio',\n",
       "       'fuzz_partial_token_set_ratio', 'fuzz_partial_token_sort_ratio',\n",
       "       'fuzz_token_set_ratio', 'fuzz_token_sort_ratio'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature set 2\n",
    "\n",
    "fs_2= ['fuzz_qratio', 'fuzz_WRatio', 'fuzz_partial_ratio',\n",
    "       'fuzz_partial_token_set_ratio', 'fuzz_partial_token_sort_ratio',\n",
    "       'fuzz_token_set_ratio', 'fuzz_token_sort_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-IDF Sparse Matrix of the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each word in Ques1 and Ques2 assigned a TF-IDF Score separately\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfv_q1 = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode', analyzer='word', \n",
    "token_pattern=r'w{1,}',ngram_range=(1, 2), use_idf=1, smooth_idf=1, sublinear_tf=1,stop_words='english')\n",
    "\n",
    "tfv_q2 = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode', analyzer='word', \n",
    "token_pattern=r'w{1,}',ngram_range=(1, 2), use_idf=1, smooth_idf=1, sublinear_tf=1,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jyoti.prakash\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['w'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "q1_tfidf = tfv_q1.fit_transform(train.question1.fillna(\"\"))\n",
    "q2_tfidf = tfv_q2.fit_transform(train.question2.fillna(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature set 3.1\n",
    "\n",
    "from scipy import sparse\n",
    "# obtain features by stacking the sparse matrices together\n",
    "fs3_1 = sparse.hstack((q1_tfidf, q2_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=3, \n",
    "                      max_features=None, \n",
    "                      strip_accents='unicode', \n",
    "                      analyzer='word', \n",
    "                      token_pattern=r'w{1,}',\n",
    "                      ngram_range=(1, 2), \n",
    "                      use_idf=1, \n",
    "                      smooth_idf=1, \n",
    "                      sublinear_tf=1,\n",
    "                      stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature set 3.2\n",
    "# combine questions and calculate tf-idf\n",
    "q1q2 = train.question1.fillna(\"\") \n",
    "q1q2 += \" \" + train.question2.fillna(\"\")\n",
    "fs3_2 = tfv.fit_transform(q1q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Model to train Word2Vec imported from Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a pretrained model for word2vec\n",
    "\n",
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "'GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jyoti.prakash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jyoti.prakash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "#Summing all the word2vecs of a question to obtain sent2vec    \n",
    "    \n",
    "def sent2vec(s,model):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    if len(M) > 0:\n",
    "        v = M.sum(axis=0)\n",
    "        return v / np.sqrt((v ** 2).sum())\n",
    "    else:\n",
    "        return np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec for q1 and q2\n",
    "\n",
    "w2v_q1 = np.array([sent2vec(q, model) \n",
    "                   for q in train.question1])\n",
    "\n",
    "w2v_q2 = np.array([sent2vec(q, model) \n",
    "                   for q in train.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jyoti.prakash\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:702: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\jyoti.prakash\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:1160: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return l1_diff.sum() / l1_sum.sum()\n"
     ]
    }
   ],
   "source": [
    "#Word2vec distances\n",
    "\n",
    "from scipy.spatial.distance import cosine, cityblock,jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "\n",
    "train['cosine_distance'] = [cosine(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "train['cityblock_distance'] = [cityblock(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "train['jaccard_distance'] = [jaccard(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "train['canberra_distance'] = [canberra(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "train['euclidean_distance'] = [euclidean(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "train['minkowski_distance'] = [minkowski(x,y,3) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "train['braycurtis_distance'] = [braycurtis(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature set 4.1\n",
    "\n",
    "fs4_1 = ['cosine_distance', 'cityblock_distance', \n",
    "         'jaccard_distance', 'canberra_distance', \n",
    "         'euclidean_distance', 'minkowski_distance',\n",
    "         'braycurtis_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = np.hstack((w2v_q1, w2v_q2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WMD use word embeddings to calculate the distance so that it can calculate even though there is no common word. The assumption is that similar words should have similar vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word movers distance\n",
    "\n",
    "def wmd(s1, s2, model):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['wmd'] = train.apply(lambda x: wmd(x['question1'], x['question2'], model), axis=1)\n",
    "model.init_sims(replace=True)  #Normalizes the vectors in word2vec class\n",
    "train['norm_wmd'] = train.apply(lambda x: wmd(x['question1'], x['question2'], model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature set 4.2\n",
    "\n",
    "fs4_2 = ['wmd', 'norm_wmd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickle dump\n",
    "\n",
    "import pickle\n",
    "filename = 'trainv2'\n",
    "infile = open(filename,'rb')\n",
    "train = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question1', 'question2', 'is_duplicate', 'len_q1', 'len_q2',\n",
       "       'len_diff', 'len_char_q1', 'len_char_q2', 'len_word_q1', 'len_word_q2',\n",
       "       'common_words', 'fuzz_qratio', 'fuzz_WRatio', 'fuzz_partial_ratio',\n",
       "       'fuzz_partial_token_set_ratio', 'fuzz_partial_token_sort_ratio',\n",
       "       'fuzz_token_set_ratio', 'fuzz_token_sort_ratio', 'cosine_distance',\n",
       "       'cityblock_distance', 'jaccard_distance', 'canberra_distance',\n",
       "       'euclidean_distance', 'minkowski_distance', 'braycurtis_distance',\n",
       "       'wmd', 'norm_wmd'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=17057210368, available=10876678144, percent=36.2, used=6180532224, free=10876678144)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning up the memory\n",
    "\n",
    "import gc\n",
    "import psutil\n",
    "#del([tfv_q1, tfv_q2, tfv, q1q2,q1_tfidf, q2_tfidf])\n",
    "del([w2v_q1, w2v_q2])\n",
    "del([model])\n",
    "gc.collect()\n",
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "y = train.is_duplicate.values\n",
    "y = y.astype('float32').reshape(-1, 1)\n",
    "X = train[fs_1+fs_2+fs4_1+fs4_2]\n",
    "X = X.replace([np.inf, -np.inf], np.nan).fillna(0).values\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the validation set\n",
    "\n",
    "np.random.seed(42)\n",
    "n_all, _ = y.shape\n",
    "idx = np.arange(n_all)\n",
    "np.random.shuffle(idx)\n",
    "n_split = n_all // 10\n",
    "idx_val = idx[:n_split]\n",
    "idx_train = idx[n_split:]\n",
    "x_train = X[idx_train]\n",
    "y_train = np.ravel(y[idx_train])\n",
    "x_val = X[idx_val]\n",
    "y_val = np.ravel(y[idx_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regr accuracy: 0.682\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "logres = linear_model.LogisticRegression(C=0.1, solver='sag', max_iter=1000)\n",
    "logres.fit(x_train, y_train)\n",
    "lr_preds = logres.predict(x_val)\n",
    "log_res_accuracy = np.sum(lr_preds == y_val) / len(y_val)\n",
    "print(\"Logistic regr accuracy: %0.3f\" % log_res_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.687385\ttrain-error:0.297709\tvalid-logloss:0.687579\tvalid-error:0.295661\n",
      "Multiple eval metrics have been passed: 'valid-error' will be used for early stopping.\n",
      "\n",
      "Will train until valid-error hasn't improved in 50 rounds.\n",
      "[100]\ttrain-logloss:0.513457\ttrain-error:0.278424\tvalid-logloss:0.513574\tvalid-error:0.27758\n",
      "[200]\ttrain-logloss:0.491254\ttrain-error:0.273098\tvalid-logloss:0.491917\tvalid-error:0.273103\n",
      "[300]\ttrain-logloss:0.483873\ttrain-error:0.270756\tvalid-logloss:0.484876\tvalid-error:0.270159\n",
      "[400]\ttrain-logloss:0.478312\ttrain-error:0.266779\tvalid-logloss:0.479858\tvalid-error:0.267216\n",
      "[500]\ttrain-logloss:0.474123\ttrain-error:0.262552\tvalid-logloss:0.476123\tvalid-error:0.264445\n",
      "[600]\ttrain-logloss:0.470899\ttrain-error:0.259999\tvalid-logloss:0.473332\tvalid-error:0.262096\n",
      "[700]\ttrain-logloss:0.46757\ttrain-error:0.257047\tvalid-logloss:0.47042\tvalid-error:0.259845\n",
      "[800]\ttrain-logloss:0.464768\ttrain-error:0.254442\tvalid-logloss:0.4681\tvalid-error:0.257346\n",
      "[900]\ttrain-logloss:0.462307\ttrain-error:0.251828\tvalid-logloss:0.46617\tvalid-error:0.255095\n",
      "[1000]\ttrain-logloss:0.46028\ttrain-error:0.250314\tvalid-logloss:0.464581\tvalid-error:0.253883\n",
      "[1100]\ttrain-logloss:0.458418\ttrain-error:0.249149\tvalid-logloss:0.463128\tvalid-error:0.25277\n",
      "[1200]\ttrain-logloss:0.456507\ttrain-error:0.247464\tvalid-logloss:0.461629\tvalid-error:0.252078\n",
      "[1300]\ttrain-logloss:0.454865\ttrain-error:0.246073\tvalid-logloss:0.460347\tvalid-error:0.250396\n",
      "[1400]\ttrain-logloss:0.453226\ttrain-error:0.244848\tvalid-logloss:0.459088\tvalid-error:0.248738\n",
      "[1500]\ttrain-logloss:0.452003\ttrain-error:0.243935\tvalid-logloss:0.458203\tvalid-error:0.248145\n",
      "Stopping. Best iteration:\n",
      "[1486]\ttrain-logloss:0.452195\ttrain-error:0.244062\tvalid-logloss:0.458349\tvalid-error:0.247873\n",
      "\n",
      "Xgb accuracy: 0.752\n"
     ]
    }
   ],
   "source": [
    "#XGBoost\n",
    "\n",
    "params = dict()\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = ['logloss', 'error']\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 4\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_val, label=y_val)\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "bst = xgb.train(params, d_train, 5000, watchlist, \n",
    "                early_stopping_rounds=50, verbose_eval=100)\n",
    "xgb_preds = (bst.predict(d_valid) >= 0.5).astype(int)\n",
    "xgb_accuracy = np.sum(xgb_preds == y_val) / len(y_val)\n",
    "print(\"Xgb accuracy: %0.3f\" % xgb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
